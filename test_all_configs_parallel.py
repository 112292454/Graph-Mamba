#!/usr/bin/env python3
"""
å¤šGPUå¹¶è¡Œæµ‹è¯•æ‰€æœ‰Benchmarké…ç½®æ–‡ä»¶
æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¤šGPUå¹¶è¡ŒåŠ é€Ÿ
"""

import os
import sys
import subprocess
import yaml
import time
from pathlib import Path
import tempfile
import shutil
import json
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ¯å—GPUçš„å¹¶å‘æ§½ä½æ•°ï¼ˆé»˜è®¤1ï¼‰ã€‚å¯é€šè¿‡ç¯å¢ƒå˜é‡ SLOTS_PER_GPU è°ƒæ•´ï¼Œä¾‹å¦‚ SLOTS_PER_GPU=2
SLOTS_PER_GPU = int(os.environ.get('SLOTS_PER_GPU', '1'))

try:
    import GPUtil
    HAS_GPUTIL = True
except ImportError:
    HAS_GPUTIL = False
    print("âš ï¸  GPUtilæœªå®‰è£…ï¼Œå°†ä½¿ç”¨å¤‡ç”¨GPUæ£€æµ‹æ–¹æ³•")

def _validate_gpu_id(gpu_id: int) -> bool:
    """é€šè¿‡å¯åŠ¨ä¸€ä¸ªæœ€å°å­è¿›ç¨‹ï¼ŒéªŒè¯æŒ‡å®šCUDA_VISIBLE_DEVICESä¸‹torchæ˜¯å¦èƒ½çœ‹åˆ°GPUã€‚"""
    env = os.environ.copy()
    env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
    try:
        p = subprocess.run(
            [sys.executable, "-c", "import torch;print(int(torch.cuda.is_available()), torch.cuda.device_count())"],
            capture_output=True, text=True, timeout=5, env=env
        )
        if p.returncode == 0:
            out = (p.stdout or "").strip()
            parts = out.split()
            if len(parts) >= 2:
                is_avail = parts[0] == '1'
                cnt_ok = parts[1].isdigit() and int(parts[1]) >= 1
                return is_avail and cnt_ok
    except Exception:
        pass
    return False

def get_available_gpus():
    """è·å–å¯ç”¨ä¸”å¯è¢«å½“å‰Pythonè¿›ç¨‹å®é™…è®¿é—®çš„GPUåˆ—è¡¨ï¼ˆé€ä¸ªæ ¡éªŒï¼‰ã€‚"""
    candidates = []
    if HAS_GPUTIL:
        try:
            gpus = GPUtil.getGPUs()
            # å…ˆæŒ‰ç©ºé—²æ¯”ä¾‹è¿‡æ»¤å€™é€‰
            candidates = [gpu.id for gpu in gpus if gpu.memoryFree / gpu.memoryTotal > 0.3]
        except Exception:
            candidates = []
    if not candidates:
        # å¤‡ç”¨ï¼šnvidia-smi åˆ—ä¸¾
        try:
            result = subprocess.run(['nvidia-smi', '-L'], capture_output=True, text=True, timeout=10)
            if result.returncode == 0:
                gpu_lines = [line for line in result.stdout.split('\n') if 'GPU' in line]
                candidates = list(range(len(gpu_lines)))
        except Exception:
            candidates = []

    # é€ä¸ªéªŒè¯
    valid = []
    for gid in candidates or [0]:
        if _validate_gpu_id(gid):
            valid.append(gid)
        else:
            print(f"âš ï¸  è·³è¿‡ä¸å¯ç”¨GPU id={gid} (torchä¸å¯è§)")

    if not valid:
        print("âš ï¸  æœªæ‰¾åˆ°å¯ç”¨GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼ï¼ˆCUDAä¸å¯è§ï¼‰")
        return []
    return valid

class ProgressManager:
    """è¿›åº¦ç®¡ç†å™¨ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œçº¿ç¨‹å®‰å…¨"""
    def __init__(self, progress_file="test_progress_parallel.json"):
        self.progress_file = progress_file
        self.completed_tasks = set()
        self.failed_tasks = {}
        self.lock = threading.Lock()
        self.load_progress()
    
    def load_progress(self):
        """åŠ è½½ä¹‹å‰çš„è¿›åº¦"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.completed_tasks = set(data.get('completed', []))
                    self.failed_tasks = data.get('failed', {})
                    print(f"ğŸ“‚ å‘ç°è¿›åº¦æ–‡ä»¶: {len(self.completed_tasks)} ä¸ªä»»åŠ¡å·²å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸  è¿›åº¦æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
        else:
            print("ğŸ†• å¼€å§‹æ–°çš„å¹¶è¡Œæµ‹è¯•")
    
    def save_progress(self):
        """ä¿å­˜å½“å‰è¿›åº¦"""
        with self.lock:
            data = {
                'completed': list(self.completed_tasks),
                'failed': self.failed_tasks
            }
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
    
    def is_completed(self, config_key):
        """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ"""
        with self.lock:
            return config_key in self.completed_tasks
    
    def mark_completed(self, config_key):
        """æ ‡è®°ä»»åŠ¡ä¸ºå·²å®Œæˆ"""
        with self.lock:
            self.completed_tasks.add(config_key)
            # å¦‚æœä¹‹å‰å¤±è´¥è¿‡ï¼Œç§»é™¤å¤±è´¥è®°å½•
            if config_key in self.failed_tasks:
                del self.failed_tasks[config_key]
            # å»¶è¿Ÿä¿å­˜ï¼Œé¿å…é¢‘ç¹IOå¡é¡¿
    
    def mark_failed(self, config_key, error_info):
        """æ ‡è®°ä»»åŠ¡ä¸ºå¤±è´¥"""
        with self.lock:
            self.failed_tasks[config_key] = {
                'error': error_info[:200] + "..." if len(error_info) > 200 else error_info,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            # å»¶è¿Ÿä¿å­˜ï¼Œé¿å…é¢‘ç¹IOå¡é¡¿
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            return len(self.completed_tasks), len(self.failed_tasks)

def modify_config_epochs(original_config_path, epochs=2):
    """ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„max_epochå‚æ•°"""
    with open(original_config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # ä¿®æ”¹epochs
    if 'optim' not in config:
        config['optim'] = {}
    config['optim']['max_epoch'] = epochs
    
    # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
    temp_dir = tempfile.mkdtemp()
    temp_config_path = os.path.join(temp_dir, os.path.basename(original_config_path))
    
    with open(temp_config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    return temp_config_path, temp_dir

def run_test(config_path, config_key, gpu_id=None, timeout=300):
    """è¿è¡Œå•ä¸ªé…ç½®æ–‡ä»¶çš„æµ‹è¯•"""
    try:
        # ä¿®æ”¹é…ç½®æ–‡ä»¶epochs
        temp_config_path, temp_dir = modify_config_epochs(config_path, epochs=2)
        
        start_time = time.time()
        
        # è®¾ç½®ç¯å¢ƒå˜é‡æŒ‡å®šGPU
        env = os.environ.copy()
        env['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
        if gpu_id is not None:
            env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)

        # è°ƒè¯•æ—¥å¿—ï¼šæ‰“å°å½“å‰ä»»åŠ¡å°†ä½¿ç”¨çš„CUDAå¯è§è®¾å¤‡
        cfg_name = os.path.basename(config_path)
        print(f"[GPU{gpu_id}] CUDA_VISIBLE_DEVICES={env.get('CUDA_VISIBLE_DEVICES', '<none>')} | cfg={cfg_name}")

        # é¢å¤–å¿«é€Ÿæ£€æŸ¥ï¼šå½“å‰ç¯å¢ƒä¸‹çš„torch.cudaå¯ç”¨æ€§
        try:
            cuda_check = subprocess.run(
                [sys.executable, "-c", "import torch;\nprint(f'CUDA_AVAIL={torch.cuda.is_available()} NUM={torch.cuda.device_count()}')"],
                capture_output=True,
                text=True,
                timeout=10,
                cwd="/home/gzy/py/Graph-Mamba",
                env=env
            )
            cuda_ok = False
            if cuda_check.returncode == 0:
                msg = cuda_check.stdout.strip()
                print(f"[GPU{gpu_id}] torch.cuda check: {msg}")
                # è§£æå¯ç”¨æ€§
                try:
                    parts = msg.replace('CUDA_AVAIL=','').replace('NUM=','').split()
                    if len(parts) >= 2:
                        avail = parts[0] in ('True','1')
                        num = int(parts[1]) if parts[1].isdigit() else 0
                        cuda_ok = (avail and num >= 1)
                except Exception:
                    cuda_ok = False
            else:
                print(f"[GPU{gpu_id}] torch.cuda check failed: {cuda_check.stderr.strip()}")
        except Exception as _e:
            print(f"[GPU{gpu_id}] torch.cuda check exception: {_e}")

        # å¼ºåˆ¶è¦æ±‚GPUå¯ç”¨
        if not locals().get('cuda_ok', False):
            return {
                'status': 'FAILED',
                'runtime': '0s',
                'result': None,
                'error': f"CUDA is not available for GPU {gpu_id} (cfg={cfg_name}). Abort."
            }
        
        # è¿è¡Œæµ‹è¯•
        cmd = [sys.executable, "main.py", "--cfg", temp_config_path, "--repeat", "1"]

        # æ‰“å°ä¸è®°å½•å°†è¦æ‰§è¡Œçš„å‘½ä»¤
        cmd_str = ' '.join(cmd)
        print(f"[GPU{gpu_id}] â–¶ CMD: {cmd_str}")

        # æ—¥å¿—æ–‡ä»¶
        logs_dir = os.path.join('results', 'parallel_test_logs')
        os.makedirs(logs_dir, exist_ok=True)
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        safe_key = config_key.replace('/', '_')
        log_path = os.path.join(logs_dir, f"{timestamp}_{safe_key}.log")
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            timeout=timeout,
            cwd="/home/gzy/py/Graph-Mamba",
            env=env
        )
        
        end_time = time.time()
        runtime = end_time - start_time
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        shutil.rmtree(temp_dir)
        
        # å°†stdout/stderrå†™å…¥æ—¥å¿—æ–‡ä»¶
        try:
            with open(log_path, 'w', encoding='utf-8') as lf:
                lf.write(f"# CMD: {cmd_str}\n")
                lf.write(f"# CUDA_VISIBLE_DEVICES={env.get('CUDA_VISIBLE_DEVICES')}\n")
                lf.write("\n# ===== STDOUT =====\n")
                lf.write(result.stdout or '')
                lf.write("\n# ===== STDERR =====\n")
                lf.write(result.stderr or '')
        except Exception:
            pass

        if result.returncode == 0:
            # ä»è¾“å‡ºä¸­æå–æœ€ç»ˆæŒ‡æ ‡
            lines = result.stdout.split('\n')
            best_line = None
            for line in lines:
                if "Best so far:" in line and "epoch 1" in line:
                    best_line = line.strip()
                    break
            
            return {
                'status': 'SUCCESS',
                'runtime': f"{runtime:.1f}s",
                'result': best_line if best_line else "è®­ç»ƒå®Œæˆ",
                'error': None,
                'log': log_path
            }
        else:
            # è¿”å›å®Œæ•´é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
            full_error = f"STDOUT:\n{result.stdout}\n\nSTDERR:\n{result.stderr}"
            return {
                'status': 'FAILED', 
                'runtime': f"{runtime:.1f}s",
                'result': None,
                'error': full_error,
                'log': log_path
            }
            
    except subprocess.TimeoutExpired:
        if 'temp_dir' in locals():
            shutil.rmtree(temp_dir)
        return {
            'status': 'TIMEOUT',
            'runtime': f"{timeout}s+",
            'result': None,
            'error': f"è¶…æ—¶ (>{timeout}s)"
        }
    except Exception as e:
        if 'temp_dir' in locals():
            shutil.rmtree(temp_dir)
        return {
            'status': 'ERROR',
            'runtime': "0s",
            'result': None,
            'error': str(e)
        }

def _torch_cuda_ok(env):
    """åœ¨ç»™å®šç¯å¢ƒå˜é‡ä¸‹å¿«é€Ÿæ£€æµ‹ torch æ˜¯å¦èƒ½çœ‹åˆ° CUDA ä¸è®¾å¤‡æ•°é‡ã€‚"""
    try:
        p = subprocess.run(
            [sys.executable, "-c", "import torch;\nprint(f'CUDA_AVAIL={torch.cuda.is_available()} NUM={torch.cuda.device_count()}')"],
            capture_output=True, text=True, timeout=10, cwd="/home/gzy/py/Graph-Mamba", env=env
        )
        if p.returncode == 0:
            msg = (p.stdout or '').strip()
            parts = msg.replace('CUDA_AVAIL=','').replace('NUM=','').split()
            if len(parts) >= 2:
                avail = parts[0] in ('True','1')
                num = int(parts[1]) if parts[1].isdigit() else 0
                return avail and num >= 1, msg
        return False, (p.stderr or '').strip()
    except Exception as e:
        return False, str(e)

def launch_task_nonblocking(config_path, config_key, gpu_id, timeout):
    """åˆ›å»ºä¸´æ—¶cfgå¹¶ä»¥éé˜»å¡å­è¿›ç¨‹æ–¹å¼å¯åŠ¨ä»»åŠ¡ï¼Œå°†è¾“å‡ºç›´æ¥å†™åˆ°å½“å‰ç»ˆç«¯ã€‚"""
    temp_config_path, temp_dir = modify_config_epochs(config_path, epochs=2)
    env = os.environ.copy()
    env['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'
    env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
    cfg_name = os.path.basename(config_path)
    print(f"[GPU{gpu_id}] CUDA_VISIBLE_DEVICES={env.get('CUDA_VISIBLE_DEVICES')} | cfg={cfg_name}")
    ok, cuda_msg = _torch_cuda_ok(env)
    print(f"[GPU{gpu_id}] torch.cuda check: {cuda_msg}")
    if not ok:
        return None, {
            'status': 'FAILED',
            'runtime': '0s',
            'result': None,
            'error': f"CUDA not available for GPU {gpu_id}: {cuda_msg}",
            'log': None
        }, temp_dir

    cmd = [sys.executable, "main.py", "--cfg", temp_config_path, "--repeat", "1"]
    cmd_str = ' '.join(cmd)
    print(f"[GPU{gpu_id}] â–¶ CMD: {cmd_str}")

    proc = subprocess.Popen(
        cmd,
        cwd="/home/gzy/py/Graph-Mamba",
        env=env,
        stdout=None,  # ç»§æ‰¿çˆ¶è¿›ç¨‹stdout
        stderr=None,  # ç»§æ‰¿çˆ¶è¿›ç¨‹stderr
        text=True
    )
    start_time = time.time()
    return {'proc': proc, 'start': start_time,
            'temp_dir': temp_dir, 'timeout': timeout, 'task': (config_path, config_key)}, None, temp_dir

def find_all_benchmark_configs():
    """æ‰¾åˆ°æ‰€æœ‰éœ€è¦æµ‹è¯•çš„é…ç½®æ–‡ä»¶"""
    benchmark_dir = Path("/home/gzy/py/Graph-Mamba/configs/Benchmark")
    
    configs = []
    
    # GPS é…ç½®æ–‡ä»¶
    gps_dir = benchmark_dir / "GPS"
    for config_file in gps_dir.glob("*-exported-GPS.yaml"):
        configs.append(("GPS", str(config_file)))
    
    # Mamba é…ç½®æ–‡ä»¶  
    mamba_dir = benchmark_dir / "Mamba"
    for config_file in mamba_dir.glob("*-exported-Mamba.yaml"):
        configs.append(("Mamba", str(config_file)))
        
    # Exphormer_LRGB é…ç½®æ–‡ä»¶
    exphormer_dir = benchmark_dir / "Exphormer_LRGB"  
    for config_file in exphormer_dir.glob("*-exported-EX.yaml"):
        configs.append(("Exphormer_LRGB", str(config_file)))
    
    return sorted(configs)

def worker_function(gpu_id, tasks, progress_manager, results_dict, worker_id):
    """Workerå‡½æ•°ï¼šåœ¨æŒ‡å®šGPUä¸Šå¤„ç†ä»»åŠ¡"""
    print(f"ğŸ”„ Worker {worker_id} å¯åŠ¨ï¼Œä½¿ç”¨GPU {gpu_id}")
    
    for task in tasks:
        model_name, config_path, dataset_name, config_key = task
        
        try:
            print(f"[GPU{gpu_id}] ğŸ”§ æµ‹è¯• {config_key}")
            
            result = run_test(config_path, config_key, gpu_id=gpu_id, timeout=300)
            
            if result['status'] == 'SUCCESS':
                progress_manager.mark_completed(config_key)
                print(f"[GPU{gpu_id}] âœ… {config_key} å®Œæˆ ({result['runtime']}) | log={result.get('log','-')}")
            else:
                progress_manager.mark_failed(config_key, result['error'])
                print(f"[GPU{gpu_id}] âŒ {config_key} å¤±è´¥ ({result['runtime']}) | log={result.get('log','-')}")
                
            results_dict[config_key] = result
            
        except Exception as e:
            error_msg = str(e)
            progress_manager.mark_failed(config_key, error_msg)
            results_dict[config_key] = {
                'status': 'ERROR',
                'runtime': '0s', 
                'result': None,
                'error': error_msg
            }
            print(f"[GPU{gpu_id}] ğŸ’¥ {config_key} å¼‚å¸¸: {error_msg}")
    
    print(f"ğŸ Worker {worker_id} (GPU {gpu_id}) å®Œæˆ")

def run_all_tests_parallel():
    """ä½¿ç”¨å¤šGPUå¹¶è¡Œè¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    # æ£€æµ‹å¯ç”¨GPU
    available_gpus = get_available_gpus()
    num_workers = min(len(available_gpus), 4)  # æœ€å¤šä½¿ç”¨4ä¸ªGPU
    print(f"ğŸ” æ£€æµ‹åˆ° {len(available_gpus)} ä¸ªå¯ç”¨GPU: {available_gpus}")
    if num_workers == 0:
        print("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨GPUï¼ˆæˆ–CUDAåœ¨å½“å‰è¿›ç¨‹ä¸å¯è§ï¼‰ï¼Œå·²ç»ˆæ­¢ã€‚")
        return False
    print(f"ğŸ’» å°†ä½¿ç”¨ {num_workers} ä¸ªworkerå¹¶è¡Œæ‰§è¡Œ")
    
    configs = find_all_benchmark_configs()
    progress = ProgressManager()
    results_dict = {}
    total_start = time.time()
    
    completed_count, failed_count = progress.get_stats()
    if completed_count > 0:
        print(f"ğŸ“Š è¿›åº¦çŠ¶æ€: å·²å®Œæˆ {completed_count} ä¸ª, å¤±è´¥ {failed_count} ä¸ª")
        if failed_count > 0:
            print("âš ï¸  ä¹‹å‰å¤±è´¥çš„ä»»åŠ¡:")
            for task, info in progress.failed_tasks.items():
                print(f"   {task}: {info['error']} ({info['timestamp']})")
        print()
    
    total_tasks = len(configs)
    remaining_tasks = []
    for model_name, config_path in configs:
        dataset_name = os.path.basename(config_path).split('-exported-')[0]
        config_key = f"{model_name}-{dataset_name}"
        if not progress.is_completed(config_key):
            remaining_tasks.append((model_name, config_path, dataset_name, config_key))
    
    print(f"ğŸ“ æ€»ä»»åŠ¡: {total_tasks}, å‰©ä½™: {len(remaining_tasks)}")
    if len(remaining_tasks) == 0:
        print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆ!")
        return True
    print()
    
    # éé˜»å¡è°ƒåº¦ï¼šæ¯ä¸ªGPUå¯é…ç½®å¤šä¸ªå¹¶å‘æ§½
    gpu_slots = {gid: [None for _ in range(SLOTS_PER_GPU)] for gid in available_gpus}
    pending = list(remaining_tasks)
    finished = 0
    total = len(pending)
    total_slots = sum(len(v) for v in gpu_slots.values())
    print(f"ğŸš€ éé˜»å¡å¤šè¿›ç¨‹è°ƒåº¦å¯åŠ¨ï¼ŒGPUæ•°={len(gpu_slots)} æ§½ä½æ€»æ•°={total_slots} (SLOTS_PER_GPU={SLOTS_PER_GPU})\n")

    def any_running():
        for slots in gpu_slots.values():
            for slot in slots:
                if slot is not None:
                    return True
        return False

    while pending or any_running():
        # å¯åŠ¨å¯ç”¨æ§½çš„æ–°ä»»åŠ¡
        for gid, slots in gpu_slots.items():
            for idx in range(len(slots)):
                if slots[idx] is None and pending:
                    task = pending.pop(0)
                    model_name, config_path, dataset_name, config_key = task
                    print(f"[GPU{gid}#slot{idx}] ğŸ”§ å¯åŠ¨ {config_key}")
                    # é’ˆå¯¹è€—æ—¶æ•°æ®é›†æé«˜è¶…æ—¶é˜ˆå€¼ï¼Œé¿å…è¯¯åˆ¤å¤±è´¥
                    key_lower = config_key.lower()
                    if 'twitter' in key_lower:
                        tmo = 1800  # Twitter é¦–ä¸ª epoch è¾ƒæ…¢
                    elif 'peptides' in key_lower:
                        tmo = 1200
                    elif 'aqsol' in key_lower:
                        tmo = 900
                    else:
                        tmo = 300
                    procRec, early_result, temp_dir = launch_task_nonblocking(config_path, config_key, gid, timeout=tmo)
                    if early_result is not None:
                        results_dict[config_key] = early_result
                        try:
                            progress.mark_failed(config_key, early_result['error'])
                        except Exception as _pe:
                            print(f"[GPU{gid}#slot{idx}] âš ï¸ è¿›åº¦è®°å½•å¤±è´¥(å¯åŠ¨å¤±è´¥): {config_key} -> {_pe}")
                        print(f"[GPU{gid}#slot{idx}] âŒ {config_key} å¯åŠ¨å¤±è´¥: {early_result['error']}")
                        try:
                            shutil.rmtree(temp_dir)
                        except Exception:
                            pass
                    else:
                        slots[idx] = procRec

        # è½®è¯¢å·²å¯åŠ¨è¿›ç¨‹
        for gid, slots in gpu_slots.items():
            for idx, rec in enumerate(list(slots)):
                if rec is None:
                    continue
                proc = rec['proc']
                ret = proc.poll()
                if ret is None:
                    # æ£€æŸ¥è¶…æ—¶
                    if time.time() - rec['start'] > rec['timeout']:
                        try:
                            proc.kill()
                        except Exception:
                            pass
                        ret = -9
                    else:
                        continue

                config_path, config_key = rec['task']
                runtime = f"{(time.time() - rec['start']):.1f}s"
                if ret == 0:
                    results_dict[config_key] = {
                        'status': 'SUCCESS', 'runtime': runtime, 'result': 'è®­ç»ƒå®Œæˆ', 'error': None, 'log': None
                    }
                    try:
                        progress.mark_completed(config_key)
                    except Exception as _pe:
                        print(f"[GPU{gid}#slot{idx}] âš ï¸ è¿›åº¦è®°å½•å¤±è´¥(å®Œæˆ): {config_key} -> {_pe}")
                    print(f"[GPU{gid}#slot{idx}] âœ… å®Œæˆ {config_key} ({runtime})")
                else:
                    results_dict[config_key] = {
                        'status': 'FAILED', 'runtime': runtime, 'result': None,
                        'error': f"Process exited with code {ret}.",
                        'log': None
                    }
                    try:
                        progress.mark_failed(config_key, results_dict[config_key]['error'])
                    except Exception as _pe:
                        print(f"[GPU{gid}#slot{idx}] âš ï¸ è¿›åº¦è®°å½•å¤±è´¥(å¤±è´¥): {config_key} -> {_pe}")
                    print(f"[GPU{gid}#slot{idx}] âŒ å¤±è´¥ {config_key} ({runtime})")

                # æ¸…ç†ä¸´æ—¶ç›®å½•
                try:
                    shutil.rmtree(rec['temp_dir'])
                except Exception:
                    pass

                slots[idx] = None
                finished += 1
                print(f"ğŸ“Š è¿›åº¦: {finished}/{total}")

        time.sleep(0.3)
    
    total_time = time.time() - total_start
    print(f"\nğŸ å¹¶è¡Œæµ‹è¯•å®Œæˆ! æ€»ç”¨æ—¶: {total_time:.1f}s")
    # å°è¯•æœ€ç»ˆä¿å­˜ä¸€æ¬¡è¿›åº¦
    try:
        progress.save_progress()
    except Exception as _e:
        print(f"âš ï¸ ä¿å­˜è¿›åº¦å¤±è´¥: {_e}")
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for result in results_dict.values() if result['status'] == 'SUCCESS')
    failed_count = len(results_dict) - success_count
    
    print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: âœ… {success_count} æˆåŠŸ, âŒ {failed_count} å¤±è´¥")
    
    if failed_count > 0:
        print("\nâŒ å¤±è´¥çš„ä»»åŠ¡:")
        for config_key, result in results_dict.items():
            if result['status'] != 'SUCCESS':
                print(f"  {config_key}: {result['status']}")
    
    return failed_count == 0

def main():
    print("=" * 80)
    print("ğŸš€ å¼€å§‹å¤šGPUå¹¶è¡Œæµ‹è¯•æ‰€æœ‰Benchmarké…ç½®æ–‡ä»¶ (æ¯ä¸ªé…ç½®è®­ç»ƒ2ä¸ªepoch)")
    print("æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œå¤šGPUå¹¶è¡ŒåŠ é€Ÿ")
    print("=" * 80)
    
    configs = find_all_benchmark_configs()
    print(f"\næ‰¾åˆ° {len(configs)} ä¸ªé…ç½®æ–‡ä»¶éœ€è¦æµ‹è¯•\n")
    
    success = run_all_tests_parallel()
    
    if success:
        print("\nâœ¨ æµ‹è¯•å®Œæˆï¼æ‰€æœ‰é…ç½®æ–‡ä»¶éƒ½èƒ½æ­£å¸¸è¿è¡Œ")
        print("ç°åœ¨å¯ä»¥åˆ›å»ºå¹¶è¡Œè®­ç»ƒè„šæœ¬äº†ï¼")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
        return 1
        
    return 0

if __name__ == "__main__":
    sys.exit(main())
