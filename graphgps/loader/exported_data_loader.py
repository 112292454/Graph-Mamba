"""
å¯¼å‡ºæ•°æ®ä¸“ç”¨åŠ è½½å™¨
åŸºäºTokenizerGraphé¡¹ç›®å¯¼å‡ºçš„æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
ç¡®ä¿ä¸åŸé¡¹ç›®100%æ•°æ®ä¸€è‡´æ€§
"""

import os
import pickle
import torch
import numpy as np
from torch_geometric.data import Data, InMemoryDataset
from torch_geometric.graphgym.config import cfg
from typing import Dict, List, Any, Optional


class ExportedDataset(InMemoryDataset):
    """
    åŠ è½½TokenizerGraphå¯¼å‡ºçš„æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
    
    æ•°æ®æ ¼å¼ï¼š
    {
        'graphs': List[Dict],      # å›¾æ•°æ®åˆ—è¡¨
        'labels': List[Any],       # æ ‡ç­¾åˆ—è¡¨  
        'splits': Dict[str, np.ndarray],  # æ•°æ®åˆ’åˆ†
        'dataset_info': Dict       # æ•°æ®é›†å…ƒä¿¡æ¯
    }
    """
    
    def __init__(self, 
                 exported_file_path: str,
                 dataset_name: str,
                 transform=None, 
                 pre_transform=None):
        """
        Args:
            exported_file_path: å¯¼å‡ºçš„pklæ–‡ä»¶è·¯å¾„
            dataset_name: æ•°æ®é›†åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        """
        self.exported_file_path = exported_file_path
        self.dataset_name = dataset_name
        
        if not os.path.exists(exported_file_path):
            raise FileNotFoundError(f"Exported data file not found: {exported_file_path}")
        
        # ä¸´æ—¶rootç›®å½•ï¼ŒInMemoryDatasetéœ€è¦
        temp_root = os.path.dirname(exported_file_path)
        super().__init__(root=temp_root, transform=transform, pre_transform=pre_transform)
        
        # åŠ è½½å¯¼å‡ºæ•°æ®
        self._load_exported_data()
        
    def _load_exported_data(self):
        """åŠ è½½å¯¼å‡ºçš„æ ‡å‡†åŒ–æ•°æ®"""
        print(f"ğŸ”„ Loading exported data: {self.dataset_name}")
        print(f"   File: {self.exported_file_path}")
        
        # åŠ è½½pklæ–‡ä»¶
        with open(self.exported_file_path, 'rb') as f:
            exported_data = pickle.load(f)
        
        # éªŒè¯æ•°æ®æ ¼å¼
        required_keys = ['graphs', 'labels', 'splits']
        for key in required_keys:
            if key not in exported_data:
                raise ValueError(f"Missing key '{key}' in exported data")
        
        graphs_data = exported_data['graphs']
        labels_data = exported_data['labels']
        splits_data = exported_data['splits']
        dataset_info = exported_data.get('dataset_info', {
            'task_type': 'classification',
            'dataset_name': self.dataset_name
        })
        
        print(f"   ğŸ“Š Loaded {len(graphs_data)} graphs")
        print(f"   ğŸ“‹ Task type: {dataset_info.get('task_type', 'unknown')}")
        
        # è½¬æ¢ä¸ºPyGæ ¼å¼
        pyg_data_list = self._convert_to_pyg_format(graphs_data, labels_data)
        
        # ä½¿ç”¨PyGçš„collateæ–¹æ³•å¤„ç†æ•°æ®
        self.data, self.slices = self.collate(pyg_data_list)
        
        # ä¿®å¤å¤šç»´æ ‡ç­¾è¢«å±•å¹³çš„é—®é¢˜
        first_label = pyg_data_list[0].y
        if first_label.dim() > 0 and len(first_label) > 1:
            # å¤šç»´æ ‡ç­¾ï¼šé‡æ–°reshapeå¹¶æ›´æ–°slices
            num_samples = len(pyg_data_list)
            label_dim = len(first_label)
            self.data.y = self.data.y.view(num_samples, label_dim)
            # æ›´æ–°slicesä¿¡æ¯ï¼šæ¯ä¸ªæ ·æœ¬å ä¸€è¡Œ
            self.slices['y'] = torch.arange(0, num_samples + 1)
        
        # è®¾ç½®æ•°æ®åˆ†å‰²
        self._setup_data_splits(splits_data)
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        self.dataset_info = dataset_info
        
        print(f"   âœ… Successfully loaded {self.dataset_name}")
    
    def _convert_to_pyg_format(self, graphs_data: List[Dict], labels_data: List[Any]) -> List[Data]:
        """
        å°†å¯¼å‡ºçš„å›¾æ•°æ®è½¬æ¢ä¸ºPyGçš„Dataæ ¼å¼
        
        å¯¼å‡ºæ ¼å¼ï¼š
        {
            'src': np.ndarray,         # æºèŠ‚ç‚¹ID
            'dst': np.ndarray,         # ç›®æ ‡èŠ‚ç‚¹ID  
            'num_nodes': int,          # èŠ‚ç‚¹æ€»æ•°
            'node_feat': np.ndarray,   # èŠ‚ç‚¹ç‰¹å¾ (N, D_node)
            'edge_feat': np.ndarray,   # è¾¹ç‰¹å¾ (E, D_edge)
        }
        """
        pyg_data_list = []
        
        for i, (graph_dict, label) in enumerate(zip(graphs_data, labels_data)):
            # éªŒè¯å›¾æ•°æ®æ ¼å¼
            required_graph_keys = ['src', 'dst', 'num_nodes', 'node_feat', 'edge_feat']
            for key in required_graph_keys:
                if key not in graph_dict:
                    raise ValueError(f"Missing key '{key}' in graph {i}")
            
            # æå–æ•°æ®
            src = graph_dict['src']
            dst = graph_dict['dst'] 
            num_nodes = graph_dict['num_nodes']
            node_feat = graph_dict['node_feat']
            edge_feat = graph_dict['edge_feat']
            
            # è½¬æ¢ä¸ºtorch tensor
            # è¾¹ç´¢å¼•ï¼šå°†src, dstç»„åˆä¸ºedge_index
            edge_index = torch.from_numpy(np.stack([src, dst], axis=0)).long()
            
            # èŠ‚ç‚¹ç‰¹å¾ï¼šç¡®ä¿2Då¹¶è½¬æ¢ä¸ºfloat32
            if node_feat.ndim == 1:
                node_feat = node_feat.reshape(-1, 1)
            x = torch.from_numpy(node_feat).float()  # è½¬æ¢ä¸ºfloat32ä»¥å…¼å®¹Linearå±‚
            
            # è¾¹ç‰¹å¾ï¼šç¡®ä¿2Då¹¶è½¬æ¢ä¸ºfloat32
            if edge_feat.ndim == 1:
                edge_feat = edge_feat.reshape(-1, 1)
            edge_attr = torch.from_numpy(edge_feat).float()  # è½¬æ¢ä¸ºfloat32ä»¥å…¼å®¹Linearå±‚
            
            # æ ‡ç­¾å¤„ç† - åªå¤„ç†å·²çŸ¥æƒ…å†µ
            if isinstance(label, np.ndarray):
                y = torch.from_numpy(label)
            elif isinstance(label, dict):
                # å¤šå±æ€§å›å½’ï¼ˆå¦‚QM9ï¼‰
                y = torch.tensor([label[k] for k in sorted(label.keys())])
            else:
                raise ValueError(f"Unsupported label type: {type(label)}. Expected np.ndarray or dict.")
            
            # ç¡®ä¿æ ‡ç­¾æ˜¯æ­£ç¡®çš„æ•°æ®ç±»å‹
            if y.dtype == torch.int64:
                pass  # åˆ†ç±»ä»»åŠ¡ä¿æŒint64
            else:
                y = y.float()  # å›å½’ä»»åŠ¡è½¬ä¸ºfloat
            
            # åˆ›å»ºPyG Dataå¯¹è±¡
            data = Data(
                x=x,
                edge_index=edge_index,
                edge_attr=edge_attr,
                y=y,
                num_nodes=num_nodes
            )
            
            pyg_data_list.append(data)
        
        return pyg_data_list
    
    def _setup_data_splits(self, splits_data: Dict[str, np.ndarray]):
        """è®¾ç½®æ•°æ®åˆ†å‰²"""
        train_idx = splits_data['train'].tolist()
        val_idx = splits_data['val'].tolist()
        test_idx = splits_data['test'].tolist()
        
        self.split_idxs = [train_idx, val_idx, test_idx]
        
        print(f"   ğŸ“‹ Data splits:")
        print(f"      Train: {len(train_idx)} samples")
        print(f"      Val:   {len(val_idx)} samples")
        print(f"      Test:  {len(test_idx)} samples")
    
    @property
    def processed_file_names(self):
        return []  # æ•°æ®å·²ç»å¯¼å‡ºå¤„ç†å¥½äº†
    
    def process(self):
        pass  # æ— éœ€é¢å¤–å¤„ç†


# æ•°æ®é›†åç§°åˆ°æ–‡ä»¶åçš„æ˜ å°„
DATASET_FILE_MAPPING = {
    # åˆ†å­æ•°æ®é›†ï¼ˆå›å½’ï¼‰
    'qm9': 'qm9_export.pkl',
    'zinc': 'zinc_export.pkl', 
    'aqsol': 'aqsol_export.pkl',
    'molhiv': 'molhiv_export.pkl',  # äºŒåˆ†ç±»ï¼Œä½†æ”¾åœ¨è¿™é‡Œæ–¹ä¾¿ç®¡ç†
    
    # TUæ•°æ®é›†ï¼ˆåˆ†ç±»ï¼‰
    'colors3': 'colors3_export.pkl',
    'proteins': 'proteins_export.pkl', 
    'dd': 'dd_export.pkl',
    'mutagenicity': 'mutagenicity_export.pkl',
    'coildel': 'coildel_export.pkl',
    'dblp': 'dblp_export.pkl',
    'twitter': 'twitter_export.pkl',
    'synthetic': 'synthetic_export.pkl',
    
    # LRGBæ•°æ®é›†
    'peptides_func': 'peptides_func_export.pkl',
    'peptides_struct': 'peptides_struct_export.pkl',
}


def load_exported_dataset(dataset_name: str, exported_data_dir: str) -> ExportedDataset:
    """
    åŠ è½½æŒ‡å®šçš„å¯¼å‡ºæ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        exported_data_dir: å¯¼å‡ºæ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        ExportedDatasetå¯¹è±¡
    """
    if dataset_name not in DATASET_FILE_MAPPING:
        available_datasets = list(DATASET_FILE_MAPPING.keys())
        raise ValueError(f"Unknown dataset '{dataset_name}'. Available: {available_datasets}")
    
    file_name = DATASET_FILE_MAPPING[dataset_name]
    file_path = os.path.join(exported_data_dir, file_name)
    
    return ExportedDataset(
        exported_file_path=file_path,
        dataset_name=dataset_name
    )


# ä¸»åŠ è½½å™¨è¦è°ƒç”¨çš„é¢„å¤„ç†å‡½æ•°
def preformat_exported_qm9(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„QM9æ•°æ®"""
    return load_exported_dataset('qm9', exported_data_dir)

def preformat_exported_zinc(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„ZINCæ•°æ®"""
    return load_exported_dataset('zinc', exported_data_dir)

def preformat_exported_aqsol(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„AQSOLæ•°æ®"""
    return load_exported_dataset('aqsol', exported_data_dir)

def preformat_exported_molhiv(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„MOLHIVæ•°æ®"""
    return load_exported_dataset('molhiv', exported_data_dir)

def preformat_exported_colors3(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„COLORS-3æ•°æ®"""
    return load_exported_dataset('colors3', exported_data_dir)

def preformat_exported_proteins(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„PROTEINSæ•°æ®"""
    return load_exported_dataset('proteins', exported_data_dir)

def preformat_exported_dd(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„DDæ•°æ®"""
    return load_exported_dataset('dd', exported_data_dir)

def preformat_exported_mutagenicity(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„Mutagenicityæ•°æ®"""
    return load_exported_dataset('mutagenicity', exported_data_dir)

def preformat_exported_coildel(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„COIL-DELæ•°æ®"""
    return load_exported_dataset('coildel', exported_data_dir)

def preformat_exported_dblp(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„DBLPæ•°æ®"""
    return load_exported_dataset('dblp', exported_data_dir)

def preformat_exported_twitter(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„TWITTERæ•°æ®"""
    return load_exported_dataset('twitter', exported_data_dir)

def preformat_exported_synthetic(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„SYNTHETICæ•°æ®"""
    return load_exported_dataset('synthetic', exported_data_dir)

def preformat_exported_peptides_func(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„Peptides-funcæ•°æ®"""
    return load_exported_dataset('peptides_func', exported_data_dir)

def preformat_exported_peptides_struct(exported_data_dir):
    """åŠ è½½å¯¼å‡ºçš„Peptides-structæ•°æ®"""
    return load_exported_dataset('peptides_struct', exported_data_dir)
