#!/usr/bin/env python3
"""
èšåˆå®éªŒç»“æœè„šæœ¬

æ­¤è„šæœ¬ä» results/ ç›®å½•èšåˆä¸åŒæ¨¡å‹ã€ä¸åŒæ•°æ®é›†çš„å®éªŒç»“æœï¼Œ
ç”Ÿæˆä¾¿äºåˆ†æçš„CSVæ–‡ä»¶å’Œé€è§†è¡¨ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/aggregate_results.py [--output-dir outputs]
    
è¾“å‡º:
    - benchmark_summary.csv: è¯¦ç»†ç»“æœè®°å½•
    - benchmark_pivot.csv: é€è§†è¡¨ï¼ˆæ¨ªè½´=æ¨¡å‹ï¼Œçºµè½´=æ•°æ®é›†Ã—æŒ‡æ ‡ï¼‰
"""

import os
import json
import sys
import argparse
from pathlib import Path
import pandas as pd
from typing import Dict, List, Tuple, Optional
import re

def parse_result_dir_name(result_dir_name: str) -> Tuple[str, str]:
    """
    è§£æç»“æœç›®å½•åï¼Œæå–æ•°æ®é›†åå’Œæ¨¡å‹å
    
    Args:
        result_dir_name: å¦‚ 'zinc-exported-GatedGCN'
        
    Returns:
        (dataset, model): å¦‚ ('zinc', 'GatedGCN')
    """
    # å¤„ç†ç‰¹æ®Šæƒ…å†µ
    if 'exported' in result_dir_name:
        parts = result_dir_name.split('-exported-')
        if len(parts) == 2:
            dataset, model = parts
            return dataset, model
    
    # å¤„ç†å…¶ä»–å¯èƒ½çš„æ¨¡å¼
    # å¦‚æœæ²¡æœ‰-exported-æ¨¡å¼ï¼Œå°è¯•å…¶ä»–åˆ†å‰²æ–¹å¼
    parts = result_dir_name.split('-')
    if len(parts) >= 2:
        # å‡è®¾æœ€åä¸€éƒ¨åˆ†æ˜¯æ¨¡å‹å
        model = parts[-1]
        dataset = '-'.join(parts[:-1])
        return dataset, model
    
    return result_dir_name, "Unknown"

def get_dataset_metric_mapping() -> Dict[str, str]:
    """
    æ ¹æ®æ•°æ®é›†ç±»å‹è¿”å›ä¸»è¦è¯„ä¼°æŒ‡æ ‡çš„æ˜ å°„
    """
    return {
        # å›å½’ä»»åŠ¡ - ä½¿ç”¨MAE
        'zinc': 'mae',
        'aqsol': 'mae', 
        'qm9': 'mae',
        'peptides-struct': 'mae',
        
        # åˆ†ç±»ä»»åŠ¡ - ä½¿ç”¨accuracy
        'dd': 'accuracy',
        'proteins': 'accuracy',
        'colors3': 'accuracy',
        'mutagenicity': 'accuracy',
        'coildel': 'accuracy',
        'dblp': 'accuracy',
        'twitter': 'accuracy',
        'synthetic': 'accuracy',
        
        # OGBä»»åŠ¡ - ä½¿ç”¨AUC
        'molhiv': 'auc',
        
        # å¤šæ ‡ç­¾ä»»åŠ¡ - ä½¿ç”¨AP
        'peptides-func': 'ap'
    }

def load_json_safely(file_path: Path) -> Optional[Dict]:
    """å®‰å…¨åœ°åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: æ— æ³•è¯»å– {file_path}: {e}")
        return None

def extract_metrics_from_result(result_dir: Path) -> Dict:
    """
    ä»ç»“æœç›®å½•æå–å…³é”®æŒ‡æ ‡
    
    ä¼˜å…ˆçº§:
    1. agg/val/best.json (éªŒè¯é›†æœ€ä½³ç»“æœ)
    2. agg/test/best.json (æµ‹è¯•é›†æœ€ä½³ç»“æœï¼Œå¦‚æœå­˜åœ¨)
    3. {seed}/test/stats.jsonçš„æœ€åä¸€è¡Œ (æµ‹è¯•é›†æœ€ç»ˆç»“æœ)
    
    Args:
        result_dir: ç»“æœç›®å½•è·¯å¾„
        
    Returns:
        åŒ…å«æŒ‡æ ‡çš„å­—å…¸
    """
    metrics = {}
    
    # ä¼˜å…ˆå°è¯•è¯»å–agg/val/best.json
    val_best_file = result_dir / "agg" / "val" / "best.json"
    if val_best_file.exists():
        data = load_json_safely(val_best_file)
        if data:
            metrics['val'] = data
    
    # å°è¯•è¯»å–agg/test/best.json  
    test_best_file = result_dir / "agg" / "test" / "best.json"
    if test_best_file.exists():
        data = load_json_safely(test_best_file)
        if data:
            metrics['test'] = data
    
    # å¦‚æœæ²¡æœ‰aggç»“æœï¼Œå°è¯•ä»seedç›®å½•è¯»å–
    if not metrics:
        # æŸ¥æ‰¾seedç›®å½•ï¼ˆé€šå¸¸æ˜¯æ•°å­—å‘½åï¼‰
        seed_dirs = [d for d in result_dir.iterdir() 
                    if d.is_dir() and d.name.isdigit()]
        
        if seed_dirs:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªseedç›®å½•
            seed_dir = seed_dirs[0]
            
            # è¯»å–test/stats.json
            test_stats_file = seed_dir / "test" / "stats.json"
            if test_stats_file.exists():
                try:
                    # è¯»å–æœ€åä¸€è¡Œä½œä¸ºæœ€ç»ˆç»“æœ
                    with open(test_stats_file, 'r') as f:
                        lines = f.readlines()
                        if lines:
                            last_line = lines[-1].strip()
                            if last_line:
                                data = json.loads(last_line)
                                metrics['test'] = data
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: è¯»å– {test_stats_file} å‡ºé”™: {e}")
    
    return metrics

def aggregate_all_results(results_dir: Path) -> List[Dict]:
    """
    èšåˆresultsç›®å½•ä¸‹æ‰€æœ‰å®éªŒç»“æœ
    """
    all_results = []
    dataset_metrics = get_dataset_metric_mapping()
    
    print("ğŸ” æ‰«æç»“æœç›®å½•...")
    
    # éå†resultsç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
    for result_dir in results_dir.iterdir():
        if not result_dir.is_dir():
            continue
            
        # è·³è¿‡ç‰¹æ®Šç›®å½•
        if result_dir.name.startswith('.') or result_dir.name == 'parallel_test_logs':
            continue
        
        print(f"ğŸ“Š å¤„ç†: {result_dir.name}")
        
        # è§£æç›®å½•åè·å–æ•°æ®é›†å’Œæ¨¡å‹
        dataset, model = parse_result_dir_name(result_dir.name)
        
        # æå–å®éªŒæŒ‡æ ‡
        metrics = extract_metrics_from_result(result_dir)
        
        if not metrics:
            print(f"   âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆç»“æœæ–‡ä»¶")
            continue
        
        # ç¡®å®šä¸»è¦è¯„ä¼°æŒ‡æ ‡
        base_dataset = dataset.split('-')[0] if '-' in dataset else dataset
        primary_metric = dataset_metrics.get(base_dataset, 'accuracy')
        
        # å¤„ç†æ¯ä¸ªsplitçš„ç»“æœ
        for split, data in metrics.items():
            if not data:
                continue
                
            # æå–å…³é”®ä¿¡æ¯
            result_record = {
                'model': model,
                'dataset': dataset,
                'split': split,
                'epoch': data.get('epoch', 0),
                'primary_metric': primary_metric,
                'primary_value': data.get(primary_metric, None)
            }
            
            # æ·»åŠ æ‰€æœ‰å¯ç”¨çš„æŒ‡æ ‡
            for metric_name in ['mae', 'accuracy', 'auc', 'ap', 'r2', 'loss']:
                if metric_name in data:
                    result_record[metric_name] = data[metric_name]
            
            # æ·»åŠ å…¶ä»–æœ‰ç”¨ä¿¡æ¯
            result_record['runtime'] = data.get('time_epoch', None)
            result_record['params'] = data.get('params', None)
            
            all_results.append(result_record)
            
            print(f"   âœ… {split}: {primary_metric}={data.get(primary_metric, 'N/A'):.4f}" 
                  if isinstance(data.get(primary_metric), (int, float)) 
                  else f"   âœ… {split}: {primary_metric}={data.get(primary_metric, 'N/A')}")
    
    print(f"\nğŸ“ˆ æ€»è®¡å¤„ç†äº† {len(all_results)} æ¡ç»“æœè®°å½•")
    return all_results

def create_pivot_table(df: pd.DataFrame, output_file: Path):
    """
    åˆ›å»ºé€è§†è¡¨ï¼šæ¨ªè½´=æ¨¡å‹ï¼Œçºµè½´=æ•°æ®é›†Ã—æŒ‡æ ‡
    """
    # åªä½¿ç”¨test splitæˆ–val splitçš„ç»“æœç”¨äºé€è§†è¡¨
    pivot_df = df[df['split'].isin(['test', 'val'])].copy()
    
    # å¦‚æœåŒæ—¶æœ‰testå’Œvalï¼Œä¼˜å…ˆä½¿ç”¨test
    if 'test' in pivot_df['split'].values:
        pivot_df = pivot_df[pivot_df['split'] == 'test']
    
    # åˆ›å»ºå¤åˆè¡Œç´¢å¼•: dataset + primary_metric
    pivot_df['dataset_metric'] = pivot_df['dataset'] + '_' + pivot_df['primary_metric']
    
    # åˆ›å»ºé€è§†è¡¨
    pivot_table = pivot_df.pivot_table(
        index='dataset_metric',
        columns='model', 
        values='primary_value',
        aggfunc='first'  # å¦‚æœæœ‰é‡å¤ï¼Œå–ç¬¬ä¸€ä¸ªå€¼
    )
    
    # ä¿å­˜é€è§†è¡¨
    pivot_table.to_csv(output_file)
    print(f"ğŸ’¾ é€è§†è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    
    # æ˜¾ç¤ºé€è§†è¡¨é¢„è§ˆ
    print(f"\nğŸ“‹ é€è§†è¡¨é¢„è§ˆ (å‰10è¡Œ):")
    print(pivot_table.head(10).to_string())
    
    if len(pivot_table) > 10:
        print(f"... (å…± {len(pivot_table)} è¡Œ)")

def main():
    parser = argparse.ArgumentParser(description='èšåˆGraph-Mambaå®éªŒç»“æœ')
    parser.add_argument('--output-dir', default='outputs', 
                        help='è¾“å‡ºç›®å½• (é»˜è®¤: outputs)')
    parser.add_argument('--results-dir', default='results',
                        help='ç»“æœç›®å½• (é»˜è®¤: results)')
    
    args = parser.parse_args()
    
    # è®¾ç½®è·¯å¾„
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    results_dir = project_root / args.results_dir
    output_dir = project_root / args.output_dir
    
    # æ£€æŸ¥resultsç›®å½•æ˜¯å¦å­˜åœ¨
    if not results_dir.exists():
        print(f"âŒ é”™è¯¯: ç»“æœç›®å½•ä¸å­˜åœ¨ - {results_dir}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ¯ Graph-Mamba å®éªŒç»“æœèšåˆ")
    print(f"ğŸ“‚ ç»“æœç›®å½•: {results_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print()
    
    try:
        # èšåˆæ‰€æœ‰ç»“æœ
        all_results = aggregate_all_results(results_dir)
        
        if not all_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å®éªŒç»“æœ")
            sys.exit(1)
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(all_results)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        summary_file = output_dir / "benchmark_summary.csv"
        df.to_csv(summary_file, index=False)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
        
        # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡
        print(f"\nğŸ“Š ç»“æœæ±‡æ€»:")
        print(f"   æ€»è®°å½•æ•°: {len(df)}")
        print(f"   æ¨¡å‹æ•°é‡: {df['model'].nunique()}")
        print(f"   æ•°æ®é›†æ•°é‡: {df['dataset'].nunique()}")
        
        model_counts = df['model'].value_counts()
        print(f"\nğŸ¤– å„æ¨¡å‹ç»“æœæ•°é‡:")
        for model, count in model_counts.items():
            print(f"   {model}: {count} æ¡")
        
        # åˆ›å»ºé€è§†è¡¨
        pivot_file = output_dir / "benchmark_pivot.csv"
        create_pivot_table(df, pivot_file)
        
        print(f"\nğŸ‰ ç»“æœèšåˆå®Œæˆï¼")
        print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - {summary_file}: è¯¦ç»†ç»“æœè®°å½•")
        print(f"   - {pivot_file}: é€è§†è¡¨ï¼ˆæ¨¡å‹Ã—æ•°æ®é›†ï¼‰")
        
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
