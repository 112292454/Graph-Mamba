#!/usr/bin/env python3
"""
èšåˆå®éªŒç»“æœè„šæœ¬

æ­¤è„šæœ¬ä» results/ ç›®å½•èšåˆä¸åŒæ¨¡å‹ã€ä¸åŒæ•°æ®é›†çš„å®éªŒç»“æœï¼Œ
ç”Ÿæˆä¾¿äºåˆ†æçš„CSVæ–‡ä»¶å’ŒmethodÃ—dataseté€è§†è¡¨ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/aggregate_results.py [--output-dir outputs]
    
è¾“å‡º:
    - benchmark_summary.csv: è¯¦ç»†ç»“æœè®°å½•
    - benchmark_pivot.csv: é€è§†è¡¨ï¼ˆæ¨ªè½´=æ¨¡å‹ï¼Œçºµè½´=æ•°æ®é›†Ã—æŒ‡æ ‡ï¼‰
    - MAE_best.csv: å›å½’ä»»åŠ¡MAEè¡¨æ ¼ï¼ˆæ¨¡å‹Ã—æ•°æ®é›†ï¼‰
    - Acc_best.csv: åˆ†ç±»ä»»åŠ¡å‡†ç¡®ç‡è¡¨æ ¼ï¼ˆæ¨¡å‹Ã—æ•°æ®é›†ï¼‰
"""

import os
import json
import sys
import argparse
from pathlib import Path
import pandas as pd
from typing import Dict, List, Tuple, Optional
import re
from collections import defaultdict

def parse_result_dir_name(result_dir_name: str) -> Tuple[str, str]:
    """
    è§£æç»“æœç›®å½•åï¼Œæå–æ•°æ®é›†åå’Œæ¨¡å‹å
    
    Args:
        result_dir_name: å¦‚ 'zinc-exported-GatedGCN'
        
    Returns:
        (dataset, model): å¦‚ ('zinc', 'GatedGCN')
    """
    # å¤„ç†ç‰¹æ®Šæƒ…å†µ
    if 'exported' in result_dir_name:
        parts = result_dir_name.split('-exported-')
        if len(parts) == 2:
            dataset, model = parts
            return dataset, model
    
    # å¤„ç†å…¶ä»–å¯èƒ½çš„æ¨¡å¼
    # å¦‚æœæ²¡æœ‰-exported-æ¨¡å¼ï¼Œå°è¯•å…¶ä»–åˆ†å‰²æ–¹å¼
    parts = result_dir_name.split('-')
    if len(parts) >= 2:
        # å‡è®¾æœ€åä¸€éƒ¨åˆ†æ˜¯æ¨¡å‹å
        model = parts[-1]
        dataset = '-'.join(parts[:-1])
        return dataset, model
    
    return result_dir_name, "Unknown"

def get_model_name_mapping() -> Dict[str, str]:
    """
    æ˜ å°„ç›®å½•åä¸­çš„æ¨¡å‹ååˆ°æ ‡å‡†æ¨¡å‹å
    """
    return {
        'GatedGCN': 'GatedGCN',
        'GPS': 'GraphGPS', 
        'EX': 'Exphormer',
        'Mamba': 'GraphMamba',
        # å¯èƒ½çš„å…¶ä»–æ˜ å°„
        'Exphormer': 'Exphormer',
        'GraphGPS': 'GraphGPS',
        'GraphMamba': 'GraphMamba'
    }

def get_model_order() -> List[str]:
    """
    è¿”å›æ¨¡å‹çš„æ’åºé¡ºåº
    """
    return ['GatedGCN', 'GraphGPS', 'Exphormer', 'GraphMamba']

def get_dataset_classification() -> Tuple[List[str], List[str]]:
    """
    è¿”å›å›å½’å’Œåˆ†ç±»æ•°æ®é›†çš„åˆ—è¡¨ï¼ŒæŒ‰ç…§æŒ‡å®šé¡ºåº
    """
    regression_datasets = ['qm9', 'zinc', 'aqsol', 'peptides-struct']
    classification_datasets = ['colors3', 'proteins', 'synthetic', 'mutagenicity', 
                             'coildel', 'dblp', 'dd', 'twitter', 'molhiv', 'peptides-func']
    return regression_datasets, classification_datasets

def get_dataset_metric_mapping() -> Dict[str, str]:
    """
    æ ¹æ®æ•°æ®é›†ç±»å‹è¿”å›ä¸»è¦è¯„ä¼°æŒ‡æ ‡çš„æ˜ å°„
    """
    return {
        # å›å½’ä»»åŠ¡ - ä½¿ç”¨MAE
        'zinc': 'mae',
        'aqsol': 'mae', 
        'qm9': 'mae',
        'peptides-struct': 'mae',
        
        # åˆ†ç±»ä»»åŠ¡ - ä½¿ç”¨accuracy
        'dd': 'accuracy',
        'proteins': 'accuracy',
        'colors3': 'accuracy',
        'mutagenicity': 'accuracy',
        'coildel': 'accuracy',
        'dblp': 'accuracy',
        'twitter': 'accuracy',
        'synthetic': 'accuracy',
        
        # OGBä»»åŠ¡ - ä½¿ç”¨AUC
        'molhiv': 'auc',
        
        # å¤šæ ‡ç­¾ä»»åŠ¡ - ä½¿ç”¨AP
        'peptides-func': 'ap'
    }

def determine_task_type(dataset: str, metric_mapping: Dict[str, str]) -> str:
    """
    æ ¹æ®æ•°æ®é›†ç¡®å®šä»»åŠ¡ç±»å‹
    """
    base_dataset = dataset.split('-')[0] if '-' in dataset else dataset
    metric = metric_mapping.get(base_dataset, metric_mapping.get(dataset, 'accuracy'))
    
    if metric == 'mae':
        return 'regression'
    else:
        return 'classification'

def load_json_safely(file_path: Path) -> Optional[Dict]:
    """å®‰å…¨åœ°åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: æ— æ³•è¯»å– {file_path}: {e}")
        return None

def extract_best_metric_value(result_dir: Path, dataset: str, metric_mapping: Dict[str, str]) -> Optional[float]:
    """
    ä»ç»“æœç›®å½•æå–æœ€ä½³æŒ‡æ ‡å€¼
    
    Args:
        result_dir: ç»“æœç›®å½•è·¯å¾„
        dataset: æ•°æ®é›†åç§°
        metric_mapping: æŒ‡æ ‡æ˜ å°„
        
    Returns:
        æœ€ä½³æŒ‡æ ‡å€¼ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
    """
    # ç¡®å®šç›®æ ‡æŒ‡æ ‡
    base_dataset = dataset.split('-')[0] if '-' in dataset else dataset
    target_metric = metric_mapping.get(base_dataset, metric_mapping.get(dataset, 'accuracy'))
    
    best_value = None
    
    # ä¼˜å…ˆå°è¯•è¯»å–agg/test/best.json (æµ‹è¯•é›†æœ€ä½³ç»“æœ)
    test_best_file = result_dir / "agg" / "test" / "best.json"
    if test_best_file.exists():
        data = load_json_safely(test_best_file)
        if data and target_metric in data:
            return data[target_metric]
    
    # å°è¯•è¯»å–agg/val/best.json (éªŒè¯é›†æœ€ä½³ç»“æœ)
    val_best_file = result_dir / "agg" / "val" / "best.json"
    if val_best_file.exists():
        data = load_json_safely(val_best_file)
        if data and target_metric in data:
            best_value = data[target_metric]
    
    # å¦‚æœæ²¡æœ‰aggç»“æœï¼Œå°è¯•ä»seedç›®å½•è¯»å–
    if best_value is None:
        # æŸ¥æ‰¾seedç›®å½•ï¼ˆé€šå¸¸æ˜¯æ•°å­—å‘½åï¼‰
        seed_dirs = [d for d in result_dir.iterdir() 
                    if d.is_dir() and d.name.isdigit()]
        
        if seed_dirs:
            # ä»æ‰€æœ‰seedä¸­æ‰¾æœ€ä½³å€¼
            seed_values = []
            for seed_dir in seed_dirs:
                # å°è¯•è¯»å–test/stats.jsonçš„æœ€åä¸€è¡Œ
                test_stats_file = seed_dir / "test" / "stats.json"
                if test_stats_file.exists():
                    try:
                        with open(test_stats_file, 'r') as f:
                            lines = f.readlines()
                            if lines:
                                last_line = lines[-1].strip()
                                if last_line:
                                    data = json.loads(last_line)
                                    if target_metric in data:
                                        seed_values.append(data[target_metric])
                    except Exception as e:
                        continue
            
            if seed_values:
                # æ ¹æ®æŒ‡æ ‡ç±»å‹é€‰æ‹©æœ€ä½³å€¼
                if target_metric == 'mae':  # è¶Šå°è¶Šå¥½
                    best_value = min(seed_values)
                else:  # accuracy, auc, ap è¶Šå¤§è¶Šå¥½
                    best_value = max(seed_values)
    
    return best_value

def extract_metrics_from_result(result_dir: Path) -> Dict:
    """
    ä»ç»“æœç›®å½•æå–å…³é”®æŒ‡æ ‡ (ä¿ç•™åŸå‡½æ•°ç”¨äºè¯¦ç»†åˆ†æ)
    """
    metrics = {}
    
    # ä¼˜å…ˆå°è¯•è¯»å–agg/val/best.json
    val_best_file = result_dir / "agg" / "val" / "best.json"
    if val_best_file.exists():
        data = load_json_safely(val_best_file)
        if data:
            metrics['val'] = data
    
    # å°è¯•è¯»å–agg/test/best.json  
    test_best_file = result_dir / "agg" / "test" / "best.json"
    if test_best_file.exists():
        data = load_json_safely(test_best_file)
        if data:
            metrics['test'] = data
    
    # å¦‚æœæ²¡æœ‰aggç»“æœï¼Œå°è¯•ä»seedç›®å½•è¯»å–
    if not metrics:
        # æŸ¥æ‰¾seedç›®å½•ï¼ˆé€šå¸¸æ˜¯æ•°å­—å‘½åï¼‰
        seed_dirs = [d for d in result_dir.iterdir() 
                    if d.is_dir() and d.name.isdigit()]
        
        if seed_dirs:
            # é€‰æ‹©ç¬¬ä¸€ä¸ªseedç›®å½•
            seed_dir = seed_dirs[0]
            
            # è¯»å–test/stats.json
            test_stats_file = seed_dir / "test" / "stats.json"
            if test_stats_file.exists():
                try:
                    # è¯»å–æœ€åä¸€è¡Œä½œä¸ºæœ€ç»ˆç»“æœ
                    with open(test_stats_file, 'r') as f:
                        lines = f.readlines()
                        if lines:
                            last_line = lines[-1].strip()
                            if last_line:
                                data = json.loads(last_line)
                                metrics['test'] = data
                except Exception as e:
                    print(f"âš ï¸  è­¦å‘Š: è¯»å– {test_stats_file} å‡ºé”™: {e}")
    
    return metrics

def collect_best_results_for_tables(results_dir: Path) -> Dict[str, Dict[str, Dict[str, float]]]:
    """
    æ”¶é›†æ‰€æœ‰å®éªŒçš„æœ€ä½³ç»“æœï¼Œç»„ç»‡æˆç”¨äºåˆ›å»ºmethodÃ—datasetè¡¨æ ¼çš„æ ¼å¼
    
    Returns:
        {task_type: {model: {dataset: best_value}}}
    """
    model_mapping = get_model_name_mapping()
    dataset_metrics = get_dataset_metric_mapping()
    
    # ç»„ç»‡ç»“æœ: {task_type: {model: {dataset: value}}}
    results = defaultdict(lambda: defaultdict(dict))
    
    print("ğŸ” æ‰«æç»“æœç›®å½•æ”¶é›†æœ€ä½³ç»“æœ...")
    
    # éå†resultsç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
    for result_dir in results_dir.iterdir():
        if not result_dir.is_dir():
            continue
            
        # è·³è¿‡ç‰¹æ®Šç›®å½•
        if result_dir.name.startswith('.') or result_dir.name == 'parallel_test_logs':
            continue
        
        print(f"ğŸ“Š å¤„ç†: {result_dir.name}")
        
        # è§£æç›®å½•åè·å–æ•°æ®é›†å’Œæ¨¡å‹
        dataset, model_raw = parse_result_dir_name(result_dir.name)
        
        # æ˜ å°„æ¨¡å‹å
        model = model_mapping.get(model_raw, model_raw)
        
        # æå–æœ€ä½³æŒ‡æ ‡å€¼
        best_value = extract_best_metric_value(result_dir, dataset, dataset_metrics)
        
        if best_value is None:
            print(f"   âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆç»“æœ")
            continue
        
        # ç¡®å®šä»»åŠ¡ç±»å‹
        task_type = determine_task_type(dataset, dataset_metrics)
        
        # å­˜å‚¨ç»“æœ
        results[task_type][model][dataset] = best_value
        
        # ç¡®å®šæŒ‡æ ‡åç§°ç”¨äºæ˜¾ç¤º
        base_dataset = dataset.split('-')[0] if '-' in dataset else dataset
        metric_name = dataset_metrics.get(base_dataset, dataset_metrics.get(dataset, 'accuracy'))
        
        print(f"   âœ… {model} - {dataset}: {metric_name}={best_value:.4f}")
    
    return dict(results)

def aggregate_all_results(results_dir: Path) -> List[Dict]:
    """
    èšåˆresultsç›®å½•ä¸‹æ‰€æœ‰å®éªŒç»“æœ (ä¿ç•™åŸå‡½æ•°ç”¨äºè¯¦ç»†åˆ†æ)
    """
    all_results = []
    dataset_metrics = get_dataset_metric_mapping()
    
    print("ğŸ” æ‰«æç»“æœç›®å½•...")
    
    # éå†resultsç›®å½•ä¸‹çš„æ‰€æœ‰å­ç›®å½•
    for result_dir in results_dir.iterdir():
        if not result_dir.is_dir():
            continue
            
        # è·³è¿‡ç‰¹æ®Šç›®å½•
        if result_dir.name.startswith('.') or result_dir.name == 'parallel_test_logs':
            continue
        
        print(f"ğŸ“Š å¤„ç†: {result_dir.name}")
        
        # è§£æç›®å½•åè·å–æ•°æ®é›†å’Œæ¨¡å‹
        dataset, model = parse_result_dir_name(result_dir.name)
        
        # æå–å®éªŒæŒ‡æ ‡
        metrics = extract_metrics_from_result(result_dir)
        
        if not metrics:
            print(f"   âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆç»“æœæ–‡ä»¶")
            continue
        
        # ç¡®å®šä¸»è¦è¯„ä¼°æŒ‡æ ‡
        base_dataset = dataset.split('-')[0] if '-' in dataset else dataset
        primary_metric = dataset_metrics.get(base_dataset, 'accuracy')
        
        # å¤„ç†æ¯ä¸ªsplitçš„ç»“æœ
        for split, data in metrics.items():
            if not data:
                continue
                
            # æå–å…³é”®ä¿¡æ¯
            result_record = {
                'model': model,
                'dataset': dataset,
                'split': split,
                'epoch': data.get('epoch', 0),
                'primary_metric': primary_metric,
                'primary_value': data.get(primary_metric, None)
            }
            
            # æ·»åŠ æ‰€æœ‰å¯ç”¨çš„æŒ‡æ ‡
            for metric_name in ['mae', 'accuracy', 'auc', 'ap', 'r2', 'loss']:
                if metric_name in data:
                    result_record[metric_name] = data[metric_name]
            
            # æ·»åŠ å…¶ä»–æœ‰ç”¨ä¿¡æ¯
            result_record['runtime'] = data.get('time_epoch', None)
            result_record['params'] = data.get('params', None)
            
            all_results.append(result_record)
            
            print(f"   âœ… {split}: {primary_metric}={data.get(primary_metric, 'N/A'):.4f}" 
                  if isinstance(data.get(primary_metric), (int, float)) 
                  else f"   âœ… {split}: {primary_metric}={data.get(primary_metric, 'N/A')}")
    
    print(f"\nğŸ“ˆ æ€»è®¡å¤„ç†äº† {len(all_results)} æ¡ç»“æœè®°å½•")
    return all_results

def create_method_dataset_tables(results: Dict[str, Dict[str, Dict[str, float]]], output_dir: Path):
    """
    åˆ›å»ºmethodÃ—datasetè¡¨æ ¼
    
    Args:
        results: {task_type: {model: {dataset: best_value}}} æ ¼å¼çš„ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    regression_datasets, classification_datasets = get_dataset_classification()
    model_order = get_model_order()
    
    for task_type, task_results in results.items():
        if task_type == 'regression':
            datasets = regression_datasets
            table_name = "MAE_best.csv"
            metric_display = "MAE"
        else:  # classification
            datasets = classification_datasets
            table_name = "Acc_best.csv" 
            metric_display = "Accuracy"
        
        print(f"\nğŸ“Š åˆ›å»º{metric_display}è¡¨æ ¼...")
        
        # åˆ›å»ºè¡¨æ ¼æ•°æ®
        table_data = []
        
        # æŒ‰ç…§æŒ‡å®šé¡ºåºå¤„ç†æ¯ä¸ªæ¨¡å‹
        for model in model_order:
            if model not in task_results:
                continue
                
            row = {'Model': model}
            model_results = task_results[model]
            
            for dataset in datasets:
                if dataset in model_results:
                    value = model_results[dataset]
                    row[dataset] = f"{value:.4f}"
                else:
                    row[dataset] = "N/A"
            
            table_data.append(row)
        
        # å¦‚æœæœ‰æ•°æ®ï¼Œåˆ›å»ºå¹¶ä¿å­˜è¡¨æ ¼
        if table_data:
            df = pd.DataFrame(table_data)
            output_file = output_dir / table_name
            df.to_csv(output_file, index=False)
            print(f"ğŸ’¾ {metric_display}è¡¨æ ¼å·²ä¿å­˜åˆ°: {output_file}")
            
            # æ˜¾ç¤ºè¡¨æ ¼é¢„è§ˆ
            print(f"\nğŸ“‹ {metric_display}è¡¨æ ¼é¢„è§ˆ:")
            print(df.to_string(index=False))
        else:
            print(f"âš ï¸  æ²¡æœ‰æ‰¾åˆ°{task_type}ä»»åŠ¡çš„æœ‰æ•ˆç»“æœ")

def create_pivot_table(df: pd.DataFrame, output_file: Path):
    """
    åˆ›å»ºé€è§†è¡¨ï¼šæ¨ªè½´=æ¨¡å‹ï¼Œçºµè½´=æ•°æ®é›†Ã—æŒ‡æ ‡ (ä¿ç•™åŸå‡½æ•°)
    """
    # åªä½¿ç”¨test splitæˆ–val splitçš„ç»“æœç”¨äºé€è§†è¡¨
    pivot_df = df[df['split'].isin(['test', 'val'])].copy()
    
    # å¦‚æœåŒæ—¶æœ‰testå’Œvalï¼Œä¼˜å…ˆä½¿ç”¨test
    if 'test' in pivot_df['split'].values:
        pivot_df = pivot_df[pivot_df['split'] == 'test']
    
    # åˆ›å»ºå¤åˆè¡Œç´¢å¼•: dataset + primary_metric
    pivot_df['dataset_metric'] = pivot_df['dataset'] + '_' + pivot_df['primary_metric']
    
    # åˆ›å»ºé€è§†è¡¨
    pivot_table = pivot_df.pivot_table(
        index='dataset_metric',
        columns='model', 
        values='primary_value',
        aggfunc='first'  # å¦‚æœæœ‰é‡å¤ï¼Œå–ç¬¬ä¸€ä¸ªå€¼
    )
    
    # ä¿å­˜é€è§†è¡¨
    pivot_table.to_csv(output_file)
    print(f"ğŸ’¾ é€è§†è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    
    # æ˜¾ç¤ºé€è§†è¡¨é¢„è§ˆ
    print(f"\nğŸ“‹ é€è§†è¡¨é¢„è§ˆ (å‰10è¡Œ):")
    print(pivot_table.head(10).to_string())
    
    if len(pivot_table) > 10:
        print(f"... (å…± {len(pivot_table)} è¡Œ)")

def main():
    parser = argparse.ArgumentParser(description='èšåˆGraph-Mambaå®éªŒç»“æœ')
    parser.add_argument('--output-dir', default='outputs', 
                        help='è¾“å‡ºç›®å½• (é»˜è®¤: outputs)')
    parser.add_argument('--results-dir', default='results',
                        help='ç»“æœç›®å½• (é»˜è®¤: results)')
    
    args = parser.parse_args()
    
    # è®¾ç½®è·¯å¾„
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    results_dir = project_root / args.results_dir
    output_dir = project_root / args.output_dir
    
    # æ£€æŸ¥resultsç›®å½•æ˜¯å¦å­˜åœ¨
    if not results_dir.exists():
        print(f"âŒ é”™è¯¯: ç»“æœç›®å½•ä¸å­˜åœ¨ - {results_dir}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(exist_ok=True)
    
    print(f"ğŸ¯ Graph-Mamba å®éªŒç»“æœèšåˆ")
    print(f"ğŸ“‚ ç»“æœç›®å½•: {results_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print()
    
    try:
        # æ”¶é›†æœ€ä½³ç»“æœç”¨äºmethodÃ—datasetè¡¨æ ¼
        best_results = collect_best_results_for_tables(results_dir)
        
        if not best_results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æœ€ä½³ç»“æœ")
            sys.exit(1)
        
        # åˆ›å»ºmethodÃ—datasetè¡¨æ ¼
        create_method_dataset_tables(best_results, output_dir)
        
        # èšåˆæ‰€æœ‰è¯¦ç»†ç»“æœï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
        print(f"\n" + "="*50)
        print("ğŸ“ˆ ç”Ÿæˆè¯¦ç»†ç»“æœåˆ†æ...")
        all_results = aggregate_all_results(results_dir)
        
        if all_results:
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(all_results)
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            summary_file = output_dir / "benchmark_summary.csv"
            df.to_csv(summary_file, index=False)
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {summary_file}")
            
            # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡
            print(f"\nğŸ“Š ç»“æœæ±‡æ€»:")
            print(f"   æ€»è®°å½•æ•°: {len(df)}")
            print(f"   æ¨¡å‹æ•°é‡: {df['model'].nunique()}")
            print(f"   æ•°æ®é›†æ•°é‡: {df['dataset'].nunique()}")
            
            model_counts = df['model'].value_counts()
            print(f"\nğŸ¤– å„æ¨¡å‹ç»“æœæ•°é‡:")
            for model, count in model_counts.items():
                print(f"   {model}: {count} æ¡")
            
            # åˆ›å»ºé€è§†è¡¨
            pivot_file = output_dir / "benchmark_pivot.csv"
            create_pivot_table(df, pivot_file)
            
            print(f"\nğŸ‰ ç»“æœèšåˆå®Œæˆï¼")
            print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"   - MAE_best.csv: å›å½’ä»»åŠ¡MAEè¡¨æ ¼ï¼ˆæ¨¡å‹Ã—æ•°æ®é›†ï¼‰")
            print(f"   - Acc_best.csv: åˆ†ç±»ä»»åŠ¡å‡†ç¡®ç‡è¡¨æ ¼ï¼ˆæ¨¡å‹Ã—æ•°æ®é›†ï¼‰")
            print(f"   - {summary_file}: è¯¦ç»†ç»“æœè®°å½•")
            print(f"   - {pivot_file}: é€è§†è¡¨ï¼ˆæ¨¡å‹Ã—æ•°æ®é›†ï¼‰")
        else:
            print(f"\nğŸ‰ methodÃ—datasetè¡¨æ ¼ç”Ÿæˆå®Œæˆï¼")
            print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            print(f"   - MAE_best.csv: å›å½’ä»»åŠ¡MAEè¡¨æ ¼ï¼ˆæ¨¡å‹Ã—æ•°æ®é›†ï¼‰")
            print(f"   - Acc_best.csv: åˆ†ç±»ä»»åŠ¡å‡†ç¡®ç‡è¡¨æ ¼ï¼ˆæ¨¡å‹Ã—æ•°æ®é›†ï¼‰")
        
    except Exception as e:
        print(f"âŒ è„šæœ¬æ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
